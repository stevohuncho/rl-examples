{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prereqs\n",
    "---\n",
    "Create your [anaconda environment](https://www.anaconda.com/download/success). For this guide I used the command `conda create -n gymnasium python=3.12.2` and then `conda activate gymnasium`.\n",
    "## Conda Cmds\n",
    "---\n",
    "```bash\n",
    "conda install conda-forge::gymnasium\n",
    "conda install conda-forge::gymnasium-box2d\n",
    "conda install conda-forge::stable-baselines3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the environment\n",
    "In this example im going to be using [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/). One of the presets for this game is `\"LunarLander-V2\"` so i will be using this when creating the environment.\n",
    "\n",
    "This process isn't training the agent at all right now we are just selecting a random action with `action = env.action_space.sample()` and taking that action with `env.step(action)`\n",
    "\n",
    "The `env.render()` allows use to see hte agent playing the game in it's own window  with `render_mode=\"human\"`\n",
    "\n",
    "This condition just checks to see if the environment has stopped and then reseting it with `env.reset()` when met\n",
    "```py\n",
    "if terminated or truncated:\n",
    "    observation, info = env.reset()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"running 500 steps of random actions\")\n",
    "for i in range(500):\n",
    "    print(f\"step {i+1}\", end=\"\\r\")\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "time.sleep(2)\n",
    "env.close() # here just because closing the window manually barely works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training requires a policy. DQN (Deep Q Network) is provided by SB3 and is compatible with the Discrete action space and the Box observation space of lunar lander. \n",
    "\n",
    "First I'm recreating the environment `env` variable only because i did `env.close()` in the last block. Then, I'm using `model.learn()` to train our model for 20k steps.\n",
    "\n",
    "The `verbose=1` will split out a lot of logs just to show that its working. Later on we will ahve more sophisticated methods for this.\n",
    "\n",
    "This training takes my pc just  on CPU around 10 minutes so feel free to lower the step count if its taking too long.\n",
    "## Logging\n",
    "Here is an example of what the default log will look like. \n",
    "\n",
    "`total_timesteps` shows how many steps it has taken so far from this attempt of training. \n",
    "\n",
    "`ep_reward_mean` shows the average reward of the agent each episode. Once being more familiar with the environment we will log more info and create graphs to gain insight on training.\n",
    "```\n",
    "----------------------------------\n",
    "| rollout/            |          |\n",
    "|    ep_len_mean      | 115      |\n",
    "|    ep_rew_mean      | -270     |\n",
    "|    exploration_rate | 0.455    |\n",
    "| time/               |          |\n",
    "|    episodes         | 10       |\n",
    "|    fps              | 47       |\n",
    "|    time_elapsed     | 24       |\n",
    "|    total_timesteps  | 1147     |\n",
    "| train/              |          |\n",
    "|    learning_rate    | 0.0001   |\n",
    "|    loss             | 1.5      |\n",
    "|    n_updates        | 261      |\n",
    "----------------------------------\n",
    "```\n",
    "## Saving\n",
    "SB3 models have built in helper functions for saving your model. Using the function below it will save the current agent into a zip file with the path you provide. \n",
    "```py\n",
    "model.save(\"dqn_lunar_lander\")\n",
    "```\n",
    "These models can then be loaded again (useful for fine tuning a model) with the function below\n",
    "```py\n",
    "model = DQN.load(\"dqn_lunar_lander\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training 20k steps of DQN\")\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=20000, log_interval=1)\n",
    "model.save(\"dqn_lunar_lander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using A Trained Model\n",
    "First I load the model trained in the above cell, and now I am able to interact with the environment based on what hte previous agent learned.\n",
    "To do so we will use `model.predict(obs, deterministic=True)`. We are passing in the obs (observation) _which is the information the environment provides to the agent based on its current state_. This function will return an action and states (states are not needed).\n",
    "\n",
    "Now instead of using a random action from before we now take the trained models prediction as our next step.\n",
    "\n",
    "Based on how many timesteps you trained your model on it should find some sort of pattern when landing, whether it is successful or not is based on how much training was done.\n",
    "\n",
    "Interacting with the environment in this manner is not training the model with as we take steps since we are not feeding any more data into the model. This allows us to run **evaluations** on the model and statistically test how fit the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(\"dqn_lunar_lander\")\n",
    "print(\"loaded trained model\")\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\") # recreating env because loading the model and the env are separate\n",
    "obs, info = env.reset()\n",
    "print(\"running 500 steps of model predictions\")\n",
    "for i in range(500):\n",
    "    print(f\"step {i+1}\", end=\"\\r\")\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "time.sleep(2)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
